### 版本:
	hive-0.13.1-cdh5.3.6
### 介绍:
数据仓库平台 将hadoop上的数据操作通SQL结合 类SQL语言HiveSQL 转换为相应的Mapreduce代码进行执行

用户接口(shell客户端 JDBC ODBC web接口) 元数据库(定义在hive中的表结构信息 默认derby 一般mysql) 解析器(HQL=>mapreduce) 数据仓库(hdfs组成的数据存储容器)
### hive安装:
内嵌模式 本地模式 *远程模式

配置信息:http://archive.cloudera.com/cdh5/cdh/5/hive-0.13.1-cdh5.3.6/

地址:http://archive.cloudera.com/cdh5/cdh/5/
##### 1.安装mysql数据库
安装完数据库（能正常使用）后:
```shell
sudo vi /etc/my.cnf
  [mysql]
  default-character-set=utf8
  [mysqld]
  character-set-server=utf8
  lower_case_table_names=1	//不区分大小写
```
创建hive元数据的mysql用户
```shell
mysqladmin -u root password root
use mysql;
select user,host,password from user;
delete from user where password="";
create user 'hive' identified by 'hive';
create database hive;
grant all privileges on hive.* to 'hive'@'%' with grant option;
FLUSH PRIVILEGES;
```
使用新用户登录然后修改数据库字符编码
```shell
alter database hive character set latin1;
```
##### 2.hive本地模式安装
```shell
tar -xvf hive-0.13.1-cdh5.3.6.tar.gz -C /opt/softs/
sudo ln -s /opt/softs/hive-0.13.1-cdh5.3.6/ /opt/hive
sudo vi /etc/profile
  export HIVE_HOME=/opt/hive
  export PATH=$PATH:$HIVE_HOME/bin
cp mysql-connector-java-5.1.38.jar $HIVE_HOME/lib/
cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml
vi $HIVE_HOME/conf/hive-site.xml
<configuration>
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://lvmama:9083</value>
    <description>该参数指定了 Hive 的数据存储目录，默认位置在 HDFS 上面的 /user/hive/warehouse 路径下。</description>
  </property>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://lvmama:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=utf8</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
  </property>
  <property>
    <name>hive.querylog.location</name>
    <value>/data/hive/querylog</value>  ${system:java.io.tmpdir}/${system:user.name}
    <description>Location of Hive run time structured log file</description>
  </property>
  <property>
    <name>hive.server2.logging.operation.log.location</name>
    <value>/data/hive/log</value>
    <description>Top level directory where operation logs are stored if logging functionality is enabled</description>
  </property>
  <property>
    <name>hive.exec.local.scratchdir</name>
    <value>/data/hive/tmp</value>
    <description>Local scratch space for Hive jobs</description>
  </property>
  <property>
    <name>hive.downloaded.resources.dir</name>
    <value>/data/hive/tmp/resources</value>
    <description>Temporary local directory for added resources in the remote file system.</description>
  </property>
</configuration>
```
运行:
```shell
schematool -initSchema -dbType mysql
hive --service metastore &
hive # 进入hive-shell
  create t(id int); # 后hdfs中会出现hive文件夹
```
hive web界面的安装:(没有hwi,待补全)
```shell
hive --service hwi &
```
hive shell 命令:
```shell
hive --service help # 查看有哪些服务
hive -H # 查看帮助信息
HQL:
  TINTINT(1byte),SMALLINT(2byte),INT(4byte),BIGINT(5byte),FLOAT(4byte),DOUBLE(8byte),BOOLEAN(-),STRING(2G)
  show describe(databases tables partition) explain(执行计划)
  create (database|schema) [if not exists] database_name [comment database_comment] [location hdfs_path] [with dbproperties (property_name=value,name=value,...)]
  eg.create database if not exists bigdata comment 'this is a test database';
  describe databse|schema [extended] database_name
  drop database|schema [if exists] database_name [restrict|cascade]
  eg.drop database bigdata;
  use database_name
```
	hive表:
		内部表:
			hive进行管理（删除表即删除数据）
		外部表:
			用户进行管理（删除表不删除数据）
		创建表的三种方式:
			1.create [external] table [if not exists] [db_name.]table_name (col1_name col1_type [comment col1_comment],...)
			[comment table_comment]
			[partitioned by(col_name col_type [comment col_comment],...)]
			[clustered by (col_name,col_name,...)[sorted by (col_name [asc|desc],...)] into num_buckets buckets]	--桶
			[row format row_format]
			[stored as file_format]
			[location hdfs_path];

 			2.create table ... AS select ...(会产生数据)
			3.create table table_name like exist_tablename
		row_format:
			delimited fields terminated by '\001'
			collection terminated by '\002'
			map keys terminated by '\003'
			lines terminated by '\004'
			null defined as '\n';
		file_format:
			sequencefile,textfile(defalt),rcfile,orc,parquet,avro
		eg1:
			1.create table test_manager(id int);
			2.create external table test_external(id int);
			3.create table test_location(id int) location '/test_location';
			4.drop table test_external;
			5.drop table test_manager;
			6.drop table test_location;
			删除表的时候，内部表不管是否指定location，均会删除文件夹，外部表一定不会
		eg2:
			1.create table customers(id int, name string, phone string) comment 'this is customers table' row format delimited fields terminated by ',' location '/customers';
			2.create table customers2 like customers;
			3.create table customers3 as select * from customers;(mapreduce操作)
		eg3:
			create table complex_table_test(id int,name string,flag boolean,score array<int>,tech map<string,string>,other struct<phone:string,email:string>)
			row format delimited fields terminated by '\;'
			collection items terminated by ','
			map keys terminated by ':'
			location 'hdfs://lvmama:8020/complex_table_test';
		eg4:hive&hbase关联
			create external table hive_users(key string, id int, name string,phone string)
			row format serde 'org.apache.hadoop.hive.hbase.HBaseSerDe'
			stored by 'org.apache.hadoop.hive.hbase.HBaseStrageHandler' with serdeproperties ('hbase.columns.mapping'=':key;f:id;f:name;f:phone')
			tblproperties('hbase.table.name'='users');
		查看表格式信息:
			describe formatted hive_users;
		查看建表信息:
			show create table hive_users;
		DDL:
			describe,drop,truncate,alter,
		导入数据:
			1.load data [local] inpath 'filepath' [overwrite] into table table_name;
			2.insert (overwrite|into) table tablename1 select_statement1 from from_statement where_statement
	eg1:
		create database test_hbase;
		use test_hbase;
		set hive.cli.print.current.db=true;(设置打印当前数据库名称)
		create table classes(classid int comment 'this is classid,not null', classname string comment 'this is class name') row format delimited fields terminated by ',';
		create table students(studentid int comment 'this is student id, not null',classid int comment 'this is classid,can set to null',studentname string) row format delimited fields terminated by ',';
		load data local inpath '/home/hadoop/datas/classes.txt' [overwrite] into table classes;
		load data local inpath '/home/hadoop/datas/students.txt' [overwrite] into table students;
		create table test1(id int);
		create table test2(id int);
		from students insert into table test1 select studentid insert overwrite table test2 select distinct classid where classid is not null;
		select命令详解:
			1.[with CommonTableExpression(,CommonTableExpression)*]	--公用的table表达式
			2.select [all|distinct]select_expr,select_expr,..	--返回的查询列表
			3.from table_reference --from语句，一般可以放在with之后，select之前
			4.[where where_condition]	--where过滤条件
			5.[group by colName(Asc|desc)]	--分组条件
			6.[order|sort by colName(Asc|desc)]	--排序条件
			7.[limit number]	--limit条件
		join命令:
			内连接,外链接,半连接,笛卡尔连接
			格式:
				left_table_reference [join type] right table_ref [join_conditionn]* ([join type] right_table_ref [join_conditionn]*)*
		导出数据:
			insert overwrite [local](local&hdfs) directory directory1 [row format row_format] [stored as file_format] select ... from ...
			eg1:
				from (select classes.classname, students.studentname from classes join students on classes.classid = students.classid) as tmp insert overwrite local directory '/home/hadoop/datas/result/' select * from tmp insert overwrite directory '/test/result/01' row format delimited fields terminated by ',' select col1,col2;
		执行sql文件并输出结果到文档:
			hive --database test -f test.sql >> result.txt
			-e "hql":在linux系统中执行hive语句
			-f "filepath":执行linux中的包含hql的文件
		HQL函数:
			UDF:支持 一个输入 一个输出
				继承类org.apache.hadoop.hive.ql.exec.UDF
				重载evaluate方法
			UDAF:支持 多个输入 一个输出
				继承类org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver
			UDTF:
			show functions;
			add jar linux_jar_path
			create [temporary] function [dbname.]function_name as class_name;
			drop [temporary] function [if exists] [dbname.]function_name;







开启服务:
	zkServer.sh start && start-all.sh && start-hbase.sh
停止服务:
	stop-hbase.sh && stop-all.sh && zkServer.sh stop
删除目录:
	rm -rf ~/hdfs/* ~/zookeeper/logs/* ~/zookeeper.out ~/zookeeper/data/* $HADOOP_HOME/logs/* /home/hadoop/hbase/tmp/* && hdfs namenode -format
hadoop日志:
	sz $HADOOP_HOME/logs/*.log

### 问题总结:
###### 问题
ls: 无法访问/opt/spark/lib/spark-assembly-*.jar: 没有那个文件或目录
###### 解决
vim $HIVE_HOME/bin/hive 将 lib/spark-assembly-*.jar 替换成 jars/*.jar
###### 问题
Mkdirs failed to create file:/XXX/XXXX
at org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:111)
###### 解决2
hive.exec.scratchdir该参数指定了 Hive 的数据临时文件目录，默认位置为 HDFS 上面的 /tmp/hive 路径下
